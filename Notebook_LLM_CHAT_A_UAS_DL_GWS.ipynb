{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üçõ Fine-Tuning LLM untuk Chatbot Kuliner & Sejarah Jogja\n",
        "\n",
        "**UAS Deep Learning 2025**\n",
        "\n",
        "---\n",
        "\n",
        "## üë• Anggota Kelompok\n",
        "| No | Nama | NIM |\n",
        "|----|------|-----|\n",
        "| 1 | Michael Ardiyanto | XXXXXXXXXX |\n",
        "| 2 | [Nama Anggota 2] | XXXXXXXXXX |\n",
        "| 3 | [Nama Anggota 3] | XXXXXXXXXX |\n",
        "| 4 | [Nama Anggota 4] | XXXXXXXXXX |\n",
        "\n",
        "**Kelas:** X (A/B/C)\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Daftar Isi\n",
        "1. Import Library\n",
        "2. Dataset & Data Preparation\n",
        "3. Tokenization & Prompt Formatting\n",
        "4. Load Pre-trained Model\n",
        "5. LoRA Configuration\n",
        "6. Training Loop\n",
        "7. Model Evaluation (LLM-based Scoring)\n",
        "8. Model Prediction & Save Results\n",
        "9. Save & Upload Model\n",
        "10. Deployment ke Hugging Face Spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install transformers datasets peft lion-pytorch accelerate sentencepiece -q\n",
        "!pip install huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, get_cosine_schedule_with_warmup\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from lion_pytorch import Lion\n",
        "\n",
        "print(\"‚úÖ Semua library berhasil diimport!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Dataset & Data Preparation\n",
        "\n",
        "Dataset berisi pasangan instruction-output tentang kuliner dan sejarah Yogyakarta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload dataset ke Colab terlebih dahulu\n",
        "# File yang dibutuhkan: dataset_jogja_train.json dan dataset_jogja_test.json\n",
        "\n",
        "from google.colab import files\n",
        "print(\"üì§ Upload file dataset_jogja_train.json dan dataset_jogja_test.json\")\n",
        "# uploaded = files.upload()  # Uncomment jika perlu upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "print(\"üìÇ Memuat dataset...\")\n",
        "\n",
        "with open('dataset_jogja_train.json', 'r', encoding='utf-8') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open('dataset_jogja_test.json', 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "print(f\"‚úÖ Dataset berhasil dimuat!\")\n",
        "print(f\"   Train: {len(train_data)} samples\")\n",
        "print(f\"   Test: {len(test_data)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cek struktur dataset (instruction-input-output)\n",
        "print(\"üìã Contoh data training:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"  Instruction: {train_data[i]['instruction']}\")\n",
        "    print(f\"  Input: {train_data[i].get('input', '-')}\")\n",
        "    print(f\"  Output: {train_data[i]['output'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert ke HuggingFace Dataset\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "test_dataset = Dataset.from_list(test_data)\n",
        "\n",
        "# Buat DataLoader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ DataLoader siap!\")\n",
        "print(f\"   Train batches: {len(train_loader)}\")\n",
        "print(f\"   Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Tokenization & Prompt Formatting\n",
        "\n",
        "Menggunakan template prompt khusus untuk model Gemma dengan format chat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Template Prompt untuk Gemma\n",
        "template_without_answer = \"<start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "template_with_answer = template_without_answer + \"{answer}<end_of_turn>\\n\"\n",
        "\n",
        "# Contoh penggunaan template\n",
        "print(\"üìù Contoh format prompt:\")\n",
        "print(\"=\"*60)\n",
        "print(template_with_answer.format(\n",
        "    question=\"Apa itu gudeg?\", \n",
        "    answer=\"Gudeg adalah makanan khas Yogyakarta berbahan dasar nangka muda.\"\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Tokenizer\n",
        "model_id = \"unsloth/gemma-2-2b-it\"\n",
        "\n",
        "print(\"üîÑ Memuat tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"‚úÖ Tokenizer siap!\")\n",
        "print(f\"   Vocab size: {len(tokenizer.get_vocab())}\")\n",
        "print(f\"   Pad token: {tokenizer.pad_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test tokenisasi\n",
        "sample_text = \"Apa rekomendasi makanan di Jogja?\"\n",
        "tokens = tokenizer.encode(sample_text, return_tensors=\"pt\")\n",
        "\n",
        "print(f\"Original: {sample_text}\")\n",
        "print(f\"Token IDs: {tokens[0].tolist()}\")\n",
        "print(f\"Decoded: {tokenizer.decode(tokens[0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Collate Function untuk padding\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function untuk memproses batch data\n",
        "    dengan padding dan attention mask\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    for item in batch:\n",
        "        text = template_with_answer.format(\n",
        "            question=item['instruction'],\n",
        "            answer=item['output']\n",
        "        )\n",
        "        texts.append(text)\n",
        "    \n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    return encoded\n",
        "\n",
        "print(\"‚úÖ Collate function siap!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Load Pre-trained Model\n",
        "\n",
        "Menggunakan model Gemma 2B yang sudah di-pretrain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Model\n",
        "print(\"üîÑ Memuat model Gemma 2B...\")\n",
        "print(\"‚è≥ Ini membutuhkan waktu beberapa menit...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model berhasil dimuat!\")\n",
        "print(f\"   Model: {model_id}\")\n",
        "print(f\"   Device: {model.device}\")\n",
        "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test inference sebelum fine-tuning\n",
        "print(\"üß™ Test model sebelum fine-tuning:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_question = \"Apa itu gudeg?\"\n",
        "prompt = template_without_answer.format(question=test_question)\n",
        "tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(tokens, max_new_tokens=50)\n",
        "\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(f\"Q: {test_question}\")\n",
        "print(f\"A: {response.split('model')[-1].strip() if 'model' in response else response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. LoRA Configuration\n",
        "\n",
        "Menggunakan LoRA (Low-Rank Adaptation) untuk fine-tuning yang efisien."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Konfigurasi LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=32,                    # Rank\n",
        "    lora_alpha=64,           # Alpha = 2 √ó rank\n",
        "    lora_dropout=0.05,       # Dropout\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA ke model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"‚úÖ LoRA berhasil diterapkan!\")\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "NUM_EPOCHS = 3\n",
        "ACCUMULATION_STEPS = 8\n",
        "LEARNING_RATE = 2e-4\n",
        "WARMUP_RATIO = 0.1\n",
        "\n",
        "total_steps = (len(train_loader) // ACCUMULATION_STEPS) * NUM_EPOCHS\n",
        "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
        "\n",
        "# Optimizer & Scheduler\n",
        "optimizer = Lion(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(\"üìä Training Configuration:\")\n",
        "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"   Total steps: {total_steps}\")\n",
        "print(f\"   Warmup steps: {warmup_steps}\")\n",
        "print(f\"   Batch size: 2\")\n",
        "print(f\"   Accumulation steps: {ACCUMULATION_STEPS}\")\n",
        "print(f\"   Effective batch size: {2 * ACCUMULATION_STEPS}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss Function\n",
        "def compute_loss(model, tokens, mask, max_length=512):\n",
        "    \"\"\"\n",
        "    Hitung loss hanya untuk bagian JAWABAN (bukan pertanyaan)\n",
        "    \"\"\"\n",
        "    tokens = tokens[:, :max_length]\n",
        "    mask = mask[:, :max_length]\n",
        "\n",
        "    x = tokens[:, :-1]\n",
        "    y = tokens[:, 1:]\n",
        "    mask = mask[:, 1:]\n",
        "\n",
        "    logits = model(x).logits\n",
        "\n",
        "    loss = F.cross_entropy(\n",
        "        logits.reshape(-1, logits.size(-1)),\n",
        "        y.reshape(-1),\n",
        "        reduction=\"none\"\n",
        "    )\n",
        "\n",
        "    masked_loss = loss * mask.reshape(-1).float()\n",
        "    return masked_loss.sum() / mask.sum().clamp(min=1)\n",
        "\n",
        "print(\"‚úÖ Loss function siap!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "print(\"=\"*60)\n",
        "print(\"üöÄ MEMULAI TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad()\n",
        "\n",
        "losses = []\n",
        "global_step = 0\n",
        "best_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nüìñ EPOCH {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    epoch_losses = []\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "    \n",
        "    for i, batch in enumerate(pbar):\n",
        "        # Format data\n",
        "        q = batch[\"instruction\"][0]\n",
        "        a = batch[\"output\"][0]\n",
        "        text = template_with_answer.format(question=q, answer=a)\n",
        "        \n",
        "        # Tokenize\n",
        "        encoded = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=False\n",
        "        )\n",
        "        \n",
        "        tokens = encoded[\"input_ids\"].to(model.device)\n",
        "        \n",
        "        # Create answer mask\n",
        "        answer_start = text.find(\"<start_of_turn>model\\n\") + len(\"<start_of_turn>model\\n\")\n",
        "        answer_tokens_start = len(tokenizer.encode(text[:answer_start])) - 1\n",
        "        mask = torch.zeros_like(tokens, dtype=torch.bool)\n",
        "        mask[0, answer_tokens_start:] = True\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = compute_loss(model, tokens, mask)\n",
        "        loss = loss / ACCUMULATION_STEPS\n",
        "        loss.backward()\n",
        "        \n",
        "        epoch_losses.append(loss.item() * ACCUMULATION_STEPS)\n",
        "        \n",
        "        # Update weights\n",
        "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            \n",
        "            avg_loss = np.mean(epoch_losses[-ACCUMULATION_STEPS:])\n",
        "            losses.append(avg_loss)\n",
        "            \n",
        "            if avg_loss < best_loss:\n",
        "                best_loss = avg_loss\n",
        "            \n",
        "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"best\": f\"{best_loss:.4f}\"})\n",
        "    \n",
        "    # Epoch summary\n",
        "    epoch_avg_loss = np.mean(epoch_losses)\n",
        "    print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
        "    print(f\"   Average Loss: {epoch_avg_loss:.4f}\")\n",
        "    print(f\"   Best Loss: {best_loss:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ TRAINING SELESAI!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total Steps: {global_step}\")\n",
        "print(f\"Final Loss: {losses[-1]:.4f}\")\n",
        "print(f\"Best Loss: {best_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Training Curve\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses, alpha=0.3, color='blue', label='Raw Loss')\n",
        "if len(losses) > 20:\n",
        "    smoothed = np.convolve(losses, np.ones(20)/20, mode='valid')\n",
        "    plt.plot(range(19, len(losses)), smoothed, color='red', label='Smoothed (MA-20)')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Progress')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curve.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"üìà Training curve tersimpan di training_curve.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Model Evaluation (LLM-based Scoring)\n",
        "\n",
        "Evaluasi model menggunakan LLM sebagai juri untuk memberikan skor 0-10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi Chat untuk inference\n",
        "def chat(question, max_new_tokens=150):\n",
        "    \"\"\"\n",
        "    Fungsi untuk chat dengan model\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    prompt = template_without_answer.format(question=question)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.15,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    if \"model\\n\" in result:\n",
        "        return result.split(\"model\\n\")[-1].strip()\n",
        "    return result.strip()\n",
        "\n",
        "print(\"‚úÖ Fungsi chat() siap!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate jawaban untuk test set\n",
        "print(\"üß™ Generating jawaban untuk test set...\")\n",
        "\n",
        "n_samples = 10  # Jumlah sample untuk evaluasi\n",
        "generated_samples = []\n",
        "reference_samples = []\n",
        "\n",
        "for i, batch in enumerate(tqdm(test_loader, total=n_samples)):\n",
        "    if i >= n_samples:\n",
        "        break\n",
        "    \n",
        "    question = batch['instruction'][0]\n",
        "    reference = batch['output'][0]\n",
        "    \n",
        "    answer = chat(question, max_new_tokens=100)\n",
        "    \n",
        "    generated_samples.append(answer)\n",
        "    reference_samples.append(reference)\n",
        "\n",
        "print(f\"‚úÖ Generated {len(generated_samples)} jawaban\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LLM-based Scoring\n",
        "import re\n",
        "\n",
        "class LLMJudge:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def score(self, question, reference, answer):\n",
        "        \"\"\"\n",
        "        Beri skor 0-10 untuk jawaban model\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"<start_of_turn>user\n",
        "Rate this answer from 0-10 based on accuracy and relevance.\n",
        "\n",
        "Question: {question}\n",
        "Reference Answer: {reference}\n",
        "Model Answer: {answer}\n",
        "\n",
        "Reply with only: SCORE: X (where X is 0-10)<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=20,\n",
        "                temperature=0.1,\n",
        "                do_sample=False\n",
        "            )\n",
        "        \n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract score\n",
        "        match = re.search(r'SCORE:\\s*(\\d+)', response, re.IGNORECASE)\n",
        "        if match:\n",
        "            score = int(match.group(1))\n",
        "            return min(max(score, 0), 10)  # Clamp to 0-10\n",
        "        \n",
        "        # Fallback: cari angka\n",
        "        nums = re.findall(r'\\b(10|[0-9])\\b', response)\n",
        "        if nums:\n",
        "            return int(nums[-1])\n",
        "        \n",
        "        return 5  # Default score\n",
        "\n",
        "judge = LLMJudge(model, tokenizer)\n",
        "print(\"‚úÖ LLM Judge siap!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluasi dengan LLM Judge\n",
        "print(\"‚öñÔ∏è Mengevaluasi jawaban model...\")\n",
        "\n",
        "scores = []\n",
        "for i in tqdm(range(len(generated_samples))):\n",
        "    question = test_dataset[i]['instruction']\n",
        "    reference = reference_samples[i]\n",
        "    answer = generated_samples[i]\n",
        "    \n",
        "    score = judge.score(question, reference, answer)\n",
        "    scores.append(score)\n",
        "\n",
        "# Statistik evaluasi\n",
        "avg_score = np.mean(scores)\n",
        "min_score = np.min(scores)\n",
        "max_score = np.max(scores)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä HASIL EVALUASI\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Rata-rata Skor: {avg_score:.2f} / 10\")\n",
        "print(f\"Skor Minimum: {min_score}\")\n",
        "print(f\"Skor Maximum: {max_score}\")\n",
        "print(f\"Total Samples: {len(scores)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Model Prediction & Save Results\n",
        "\n",
        "Generate prediksi untuk test set dan simpan ke file JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pengujian 5 Skenario\n",
        "print(\"=\"*70)\n",
        "print(\"üìã PENGUJIAN 5 SKENARIO CHATBOT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_scenarios = [\n",
        "    \"Apa itu gudeg kering?\",\n",
        "    \"Dimana lokasi Sate Klathak Pak Bari?\",\n",
        "    \"Siapa pendiri Muhammadiyah?\",\n",
        "    \"Kapan Perang Diponegoro terjadi?\",\n",
        "    \"Rekomendasi kuliner malam di Jogja?\"\n",
        "]\n",
        "\n",
        "scenario_results = []\n",
        "\n",
        "for i, question in enumerate(test_scenarios, 1):\n",
        "    answer = chat(question, max_new_tokens=150)\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üß™ SKENARIO {i}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"‚ùì Pertanyaan: {question}\")\n",
        "    print(f\"ü§ñ Jawaban: {answer}\")\n",
        "    \n",
        "    scenario_results.append({\n",
        "        \"skenario\": i,\n",
        "        \"pertanyaan\": question,\n",
        "        \"jawaban\": answer\n",
        "    })\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ PENGUJIAN 5 SKENARIO SELESAI\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simpan hasil prediksi ke JSON\n",
        "print(\"üíæ Menyimpan hasil prediksi...\")\n",
        "\n",
        "predictions = []\n",
        "for i in range(len(generated_samples)):\n",
        "    predictions.append({\n",
        "        \"instruction\": test_dataset[i]['instruction'],\n",
        "        \"input\": test_dataset[i].get('input', ''),\n",
        "        \"expected_output\": reference_samples[i],\n",
        "        \"model_response\": generated_samples[i],\n",
        "        \"score\": scores[i] if i < len(scores) else None\n",
        "    })\n",
        "\n",
        "with open(\"predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(predictions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"‚úÖ {len(predictions)} prediksi tersimpan di predictions.json\")\n",
        "\n",
        "# Simpan skenario\n",
        "with open(\"test_scenarios.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(scenario_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úÖ Hasil skenario tersimpan di test_scenarios.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Save & Upload Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simpan model ke local\n",
        "output_dir = \"model_jogja_kuliner_sejarah\"\n",
        "\n",
        "print(f\"üíæ Menyimpan model ke '{output_dir}'...\")\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(\"‚úÖ Model berhasil disimpan!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login ke Hugging Face\n",
        "from huggingface_hub import login\n",
        "\n",
        "print(\"üîê Login ke Hugging Face...\")\n",
        "print(\"Dapatkan token di: https://huggingface.co/settings/tokens\")\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload model ke Hugging Face Hub\n",
        "HF_USERNAME = \"JSpiritGG\"  # Ganti dengan username kamu\n",
        "REPO_NAME = \"jogja-kuliner-chatbot\"\n",
        "\n",
        "print(\"üì§ Uploading model ke Hugging Face Hub...\")\n",
        "print(\"‚è≥ Ini membutuhkan waktu beberapa menit...\")\n",
        "\n",
        "model.push_to_hub(f\"{HF_USERNAME}/{REPO_NAME}\")\n",
        "print(\"‚úÖ Model uploaded!\")\n",
        "\n",
        "tokenizer.push_to_hub(f\"{HF_USERNAME}/{REPO_NAME}\")\n",
        "print(\"‚úÖ Tokenizer uploaded!\")\n",
        "\n",
        "print(f\"\\nüéâ Model tersimpan di: https://huggingface.co/{HF_USERNAME}/{REPO_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 10. Deployment ke Hugging Face Spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "HF_USERNAME = \"JSpiritGG\"\n",
        "SPACE_NAME = \"jogja-kuliner-chatbot\"\n",
        "\n",
        "# README untuk Space\n",
        "readme_content = '''---\n",
        "title: Jogja Kuliner Chatbot\n",
        "emoji: üçõ\n",
        "colorFrom: yellow\n",
        "colorTo: red\n",
        "sdk: gradio\n",
        "sdk_version: \"5.9.1\"\n",
        "app_file: app.py\n",
        "pinned: false\n",
        "---\n",
        "\n",
        "# üçõ Asisten Kuliner & Sejarah Jogja\n",
        "\n",
        "Chatbot berbasis **Gemma 2B** yang di-fine-tune dengan teknik **LoRA**.\n",
        "\n",
        "## üß™ Contoh Pertanyaan\n",
        "- \"Apa itu gudeg?\"\n",
        "- \"Rekomendasi kuliner malam di Jogja?\"\n",
        "- \"Siapa pendiri Muhammadiyah?\"\n",
        "'''\n",
        "\n",
        "with open(\"README.md\", \"w\") as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "# App.py untuk Gradio\n",
        "app_code = '''import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "print(\"üîÑ Memuat model...\")\n",
        "\n",
        "base_model_id = \"unsloth/gemma-2-2b-it\"\n",
        "adapter_id = \"JSpiritGG/jogja-kuliner-chatbot\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(adapter_id)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"cpu\",\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, adapter_id)\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ Model siap!\")\n",
        "\n",
        "template = \"<start_of_turn>user\\\\n{question}<end_of_turn>\\\\n<start_of_turn>model\\\\n\"\n",
        "\n",
        "def chat(message, history):\n",
        "    prompt = template.format(question=message)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.15,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"model\\\\n\" in response:\n",
        "        response = response.split(\"model\\\\n\")[-1].strip()\n",
        "    return response\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat,\n",
        "    title=\"üçõ Asisten Kuliner & Sejarah Jogja\",\n",
        "    description=\"Chatbot Fine-tuned Gemma 2B dengan LoRA - UAS Deep Learning\",\n",
        "    examples=[\"Apa itu gudeg?\", \"Rekomendasi kuliner malam di Jogja?\", \"Siapa pendiri Muhammadiyah?\"],\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "demo.launch()\n",
        "'''\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "# Requirements\n",
        "requirements = \"\"\"torch\n",
        "transformers>=4.40.0\n",
        "accelerate\n",
        "gradio\n",
        "peft\n",
        "sentencepiece\n",
        "protobuf\n",
        "\"\"\"\n",
        "\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements)\n",
        "\n",
        "print(\"‚úÖ File untuk deployment siap!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload ke Space\n",
        "api = HfApi()\n",
        "\n",
        "print(\"üì§ Uploading ke Hugging Face Space...\")\n",
        "\n",
        "for filename in [\"README.md\", \"app.py\", \"requirements.txt\"]:\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=filename,\n",
        "        path_in_repo=filename,\n",
        "        repo_id=f\"{HF_USERNAME}/{SPACE_NAME}\",\n",
        "        repo_type=\"space\"\n",
        "    )\n",
        "    print(f\"‚úÖ {filename} uploaded\")\n",
        "\n",
        "print(f\"\"\"\n",
        "{'='*60}\n",
        "üéâ DEPLOYMENT SELESAI!\n",
        "{'='*60}\n",
        "\n",
        "üîó Link Aplikasi (PERMANEN):\n",
        "   https://huggingface.co/spaces/{HF_USERNAME}/{SPACE_NAME}\n",
        "\n",
        "‚è≥ Tunggu 5-10 menit untuk build selesai.\n",
        "{'='*60}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìä Ringkasan Hasil\n",
        "\n",
        "| Metrik | Nilai |\n",
        "|--------|-------|\n",
        "| Total Training Steps | - |\n",
        "| Final Loss | - |\n",
        "| Best Loss | - |\n",
        "| Evaluation Score | - / 10 |\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Links\n",
        "- **Demo App:** https://huggingface.co/spaces/JSpiritGG/jogja-kuliner-chatbot\n",
        "- **Model:** https://huggingface.co/JSpiritGG/jogja-kuliner-chatbot\n",
        "\n",
        "---\n",
        "\n",
        "**¬© 2025 - UAS Deep Learning**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
